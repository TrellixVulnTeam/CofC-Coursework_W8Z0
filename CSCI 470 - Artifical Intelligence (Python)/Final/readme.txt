AI Final Assignment Readme
Task 1
	Task one was rather easy. I played around with a few different implementations. First instance was to just comment out all of the player move in startGame() options except for getRandomMove(board, parser). After running that it seemed too simplistic, so I then decided to create a separate AI class for the player, named Player_AI.py. Next I created a new object underneath opponentAI called playerAI and called the getRandomMove() method from the Player_AI class. Performance wise, the outcome was either a statemate or checkmate.
Task 2
	Task two has proven to be more difficult than I expected. I haven’t been able to implement a functioning alpha-beta pruning algorithm to beat the AI’s minimax, but believe I am close. Many different attempts were performed. At first I noticed the the minChildrenOfNode() and maxChildrenOfNode() methods and believed I could just add an alpha and a beta in there and be done with it. No luck, since the built in AI doesn’t seem to use them even. After reading his blog post again, it became clear to me that the minimax algorithm was hidden in the method getOptimalPointAdvantageForNode(). Where and how to add the pruning has been hours of trial, error, and messy code. First attempt was to actually modify the min/maxChildrenOfNode() functions, but that was getting me nowhere. Numerous times I had to erase everything and start from scratch. I traced back every method the built in AI was using and implemented the same for the player’s AI to get a feel for all of the moving parts. That created an endless game of king vs king, but I had a feeling I was getting somewhere. After a few more hours, and on the verge of giving up, I read the email you sent out again. I then just literally copied the pacman implementation and did the best I could to make it work. I added the functions maxValue() and minValue() both with the parameter’s self, node, alpha, and beta. The functions I modified were getOptimalPointAdvantageForNodeAB(), bestMoveWithMoveTree(), and getBestMoveAlphaBeta(). So far I can still only reach a statemate or the dueling kings again with depth of 1 and seem to still be losing with a depth of 2. No errors are being thrown and after running the debugger it looks like my pruning functions are being used. It seems to be returning the max value for the player’s AI. So frustrating because I believe I am very close to getting this program to work properly and am kicking myself for not getting this far while I was still Charleston to run it by you ahead of time. 
Task 3
	As expected from my write up for task 2, I underestimated this assignment and was unable to attempt the implementation of another AI algorithm. It would have been interesting to build a better agent using qLearning or classification, but wrapping my head around the new chess environment took longer than anticipated. I may come back to it in near future, but honestly I feel like I’m out of gas for the semester. I’m still amazed I was able to get this far with all that’s going on at home. Thank you for all of your guidance this semester and during the past three years. Now that it’s over it doesn’t feel like such a long time. I can’t believe I’m finally graduating and I look forward to seeing you on the 12th.
Neal Sakash
Dr. Anderson
CSCI 470
4/30/18

